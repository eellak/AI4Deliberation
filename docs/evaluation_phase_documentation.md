# Summary Evaluation Phase Documentation

## Overview

The evaluation phase assesses the quality of summaries generated by the Gemma 3 27B IT pipeline using Google Gemini 2.5 Pro or OpenAI models. This phase provides structured feedback on five key criteria to measure summary effectiveness for citizen consumption.

## Evaluation Model

### Primary: Google Gemini 2.5 Pro
- **Provider**: Google Generative AI
- **Context Window**: 1,048,576 tokens (1M+)
- **Temperature**: 0.0 (deterministic)
- **Top-k**: 1 (most likely token)
- **Top-p**: 0.0 (no nucleus sampling)

### Alternative: OpenAI Models
- **GPT-4**: Full evaluation capability
- **GPT-4-mini**: Cost-effective option
- **Context Management**: Automatic chunking for smaller windows

## Evaluation Process

### Input Requirements

1. **Summary Files**:
   - Pattern: `cons{N}_final_summary.txt`
   - Location: Specified summaries directory
   - Format: Plain text with part divisions

2. **Original Legislation**:
   - Primary source: `dry_run_cons_{N}.txt` files
   - Fallback: SQLite database query
   - Content: Full article text in order

3. **Consultation Metadata**:
   - Title from database (optional)
   - Consultation ID for tracking

### Evaluation Modes

#### 1. Two-Stage Evaluation (Default)

**Stage 1: Context Loading**
```
Prompt: "You are an AI assistant specializing in legislative text analysis.
I will provide you with an original legislative document.
Please carefully read, analyze, and understand this document..."

Input: {full_legislation_text}
```

**Stage 2: Summary Evaluation**
```
Prompt: "Now, please evaluate the following [Generated Summary]
based on the original legislative document you have already processed..."

Input: {summary_text}
Output: JSON evaluation scores
```

**Advantages**:
- Better context retention
- Clearer separation of tasks
- More reliable for long documents

#### 2. Single-Stage Evaluation

**Combined Prompt**:
```
Prompt: "Original Legislative Document:
{legislation_text}

[Generated Summary]:
{summary_text}

Evaluate the summary based on the original document..."
```

**Use Cases**:
- Shorter documents
- Models with limited conversation memory
- Batch processing needs

## Evaluation Criteria

### 1. Comprehension & Readability (1-5)

**Purpose**: Ensure the summary is understandable by non-experts

**Sub-criteria**:
- **Clarity**: Clear, unambiguous language
- **Plain Language**: Avoidance of unnecessary jargon
- **Tone Appropriateness**: Suitable for general citizens
- **Information Flow**: Logical progression of ideas

**Scoring Guide**:
- 5: Exceptionally clear and accessible
- 4: Very readable with minor issues
- 3: Generally understandable
- 2: Significant clarity problems
- 1: Largely incomprehensible

### 2. Factual Accuracy (1-5)

**Purpose**: Ensure fidelity to the original legislative text

**Sub-criteria**:
- **Factual Precision**: Correct legal references, article numbers, outcomes
- **No Hallucinations**: No invented information
- **Attribution Clarity**: Clear sourcing when needed

**Scoring Guide**:
- 5: Perfect accuracy, no errors
- 4: Minor inaccuracies that don't affect understanding
- 3: Some errors but core facts correct
- 2: Significant factual problems
- 1: Mostly inaccurate or fabricated

### 3. Content Coverage & Prioritization (1-5)

**Purpose**: Preserve core legislative intent and key provisions

**Sub-criteria**:
- **Core Subject Identification**: Main topics covered
- **Purpose Communication**: Why the law exists
- **Discussion Points Coverage**: Key provisions and implications
- **Outcome Reporting**: What was decided

**Scoring Guide**:
- 5: Comprehensive coverage with excellent prioritization
- 4: Good coverage, minor omissions
- 3: Adequate coverage of main points
- 2: Missing significant content
- 1: Fails to convey core message

### 4. Civic Transparency & Neutrality (1-5)

**Purpose**: Promote informed public without bias

**Sub-criteria**:
- **Objectivity**: Neutral tone
- **Balance**: Fair representation of all aspects
- **No Interpretive Bias**: Avoids editorial commentary

**Scoring Guide**:
- 5: Perfectly neutral and transparent
- 4: Generally neutral with minor tone issues
- 3: Mostly balanced
- 2: Noticeable bias or opacity
- 1: Heavily biased or misleading

### 5. Communication Efficiency (1-5)

**Purpose**: Deliver maximum value with minimum length

**Sub-criteria**:
- **Conciseness**: No unnecessary verbosity
- **Information Density**: High signal-to-noise ratio
- **Skimmability**: Easy to quickly understand

**Scoring Guide**:
- 5: Exceptionally efficient communication
- 4: Very concise with good density
- 3: Reasonable efficiency
- 2: Somewhat verbose or sparse
- 1: Very inefficient use of space

## Output Format

### CSV Structure

```csv
consultation_id,consultation_title,comprehension_and_readability,factual_accuracy,content_coverage_and_prioritization,civic_transparency_and_neutrality,communication_efficiency,timestamp,raw_json_response
1,"Τίτλος Διαβούλευσης",3,2,2,3,3,"2024-12-08 10:30:45","{...full JSON response...}"
```

### JSON Response Format

```json
{
  "comprehension_and_readability": {
    "reasoning": "The summary uses clear language but includes some undefined legal terms...",
    "score": 3
  },
  "factual_accuracy": {
    "reasoning": "Several article numbers are incorrect, and there are references to provisions not in the original...",
    "score": 2
  },
  "content_coverage_and_prioritization": {
    "reasoning": "Major sections are covered but the prioritization misses key citizen impacts...",
    "score": 2
  },
  "civic_transparency_and_neutrality": {
    "reasoning": "The tone remains neutral throughout with no editorial bias...",
    "score": 3
  },
  "communication_efficiency": {
    "reasoning": "The summary is reasonably concise but could be more efficiently structured...",
    "score": 3
  }
}
```

## Technical Implementation

### Error Handling

1. **Missing Files**:
   - Skips consultations without summaries
   - Logs warnings for missing legislation

2. **API Failures**:
   - Retry logic with exponential backoff
   - Graceful degradation to partial results

3. **JSON Parsing**:
   - Multiple parsing strategies
   - Fallback to string extraction
   - Default scores of 0 on complete failure

### Performance Optimization

1. **Token Management**:
   - Automatic chunking for long documents
   - Context window awareness
   - Efficient prompt construction

2. **Batch Processing**:
   - Processes multiple consultations sequentially
   - Progress logging
   - Partial result saving

3. **Provider Flexibility**:
   - Easy switching between Gemini and OpenAI
   - Consistent output format
   - Provider-specific optimizations

## Usage Examples

### Basic Evaluation
```bash
python -m summary_evaluation.evaluate \
    --summaries-dir outputs \
    --legislation-dir /mnt/data/AI4Deliberation \
    --out evaluation_results.csv
```

### With Specific Provider
```bash
python -m summary_evaluation.evaluate \
    --summaries-dir outputs \
    --legislation-dir /mnt/data/AI4Deliberation \
    --out results_openai.csv \
    --provider openai \
    --model-name gpt-4
```

### Single-Stage Mode
```bash
python -m summary_evaluation.evaluate \
    --summaries-dir outputs \
    --legislation-dir /mnt/data/AI4Deliberation \
    --out results_1stage.csv \
    --mode 1stage
```

## Key Insights from Evaluations

Based on actual evaluation runs, common issues include:

1. **Low Factual Accuracy**: Many summaries contain hallucinations or incorrect references
2. **Poor Content Coverage**: Key legislative provisions often missing
3. **Comprehension Issues**: Legal jargon not adequately simplified
4. **Efficiency Problems**: Summaries either too verbose or missing critical details

These insights suggest areas for improvement in the summarization pipeline, particularly in:
- Fact verification mechanisms
- Content selection algorithms
- Plain language generation
- Compression strategies