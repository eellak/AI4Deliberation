#!/usr/bin/env python3
"""
Test script to verify that article content can be perfectly reconstructed
from the chunks generated by article_parser_utils.extract_all_main_articles_with_content.
"""

import sqlite3
import logging
import os
import csv # Added for CSV reading
import random # Added for random selection

# Assuming article_parser_utils is in the same directory or accessible in PYTHONPATH
from article_parser_utils import extract_all_main_articles_with_content

# --- Configuration ---
# Use the database that we confirmed has data
DB_PATH = "/mnt/data/AI4Deliberation/new_html_extraction/deliberation_data_gr_markdownify.db"
TABLE_NAME = "articles"
CONTENT_COLUMN_NAME = "content"
ID_COLUMN_NAME = "id"
CRAMMED_ARTICLES_CSV_PATH = "/mnt/data/AI4Deliberation/new_html_extraction/article_extraction_analysis/crammed_articles_integrity_report_enhanced.csv" # Added CSV path
TOTAL_ARTICLES_TO_TEST = 20 # Target number of articles to test
MAX_CRAMMED_TO_INCLUDE = 10 # Max articles to pick from the CSV

# Configure logging
LOG_FORMAT = '%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'
logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)


def fetch_article_content_by_id(conn, article_db_id):
    """Fetches the content of a specific article by its DB ID."""
    cursor = conn.cursor()
    try:
        query = f"SELECT {CONTENT_COLUMN_NAME} FROM {TABLE_NAME} WHERE {ID_COLUMN_NAME} = ?"
        cursor.execute(query, (article_db_id,))
        row = cursor.fetchone()
        if row:
            return row[0]
        else:
            logging.warning(f"Article content not found for DB ID: {article_db_id}")
            return None
    except sqlite3.Error as e:
        logging.error(f"Database error fetching content for DB ID {article_db_id}: {e}")
        return None

def test_reconstruction(article_db_id, original_content):
    """
    Tests if the original_content can be reconstructed from chunks.
    """
    if original_content is None:
        logging.warning(f"Skipping reconstruction test for DB ID {article_db_id} due to None content.")
        return False, None, None # Indicate failure but also that original was None

    logging.info(f"--- Testing Reconstruction for Article DB ID: {article_db_id} ---")
    
    chunks = extract_all_main_articles_with_content(original_content)
    
    if not chunks:
        # If original_content was not empty but extract_all_main_articles_with_content returned no chunks,
        # this could be a problem unless original_content was truly empty (e.g., "" or just newlines that result in empty lines list).
        # The modified extract_all_main_articles_with_content should return a preamble chunk for non-empty content without article headers.
        # If original_content was "" -> lines=[] -> chunks=[] -> reconstructed="". This is correct.
        # If original_content was "\n" -> lines=[''] -> chunks=[{'type':'preamble', 'content_text':'', ...}] -> reconstructed="". This is also correct.
        logging.info(f"No chunks extracted for DB ID {article_db_id}. Original content might be effectively empty.")
        reconstructed_content = ""
    else:
        reconstructed_content = "\n".join(chunk['content_text'] for chunk in chunks)

    # Normalize original content by splitting and rejoining lines, similar to how chunks are processed and joined.
    # This handles cases where original content might have different newline styles (e.g. \r\n) or trailing newlines
    # not perfectly matched by the reconstruction of lines.splitlines() and then \n.join().
    normalized_original_content = "\n".join(original_content.splitlines())

    if normalized_original_content == reconstructed_content:
        logging.info(f"SUCCESS: Article DB ID {article_db_id} reconstructed perfectly.")
        return True, normalized_original_content, reconstructed_content
    else:
        logging.error(f"FAILURE: Article DB ID {article_db_id} reconstruction mismatch.")
        # For debugging, print original and reconstructed if they differ significantly
        # To avoid overwhelming logs, maybe just print lengths or a snippet.
        logging.debug(f"Original (normalized, len {len(normalized_original_content)}):\n'''{normalized_original_content[:500]}...'''")
        logging.debug(f"Reconstructed (len {len(reconstructed_content)}):\n'''{reconstructed_content[:500]}...'''")
        
        # Further debugging: Check line by line if lengths are similar
        original_lines_for_debug = normalized_original_content.splitlines()
        reconstructed_lines_for_debug = reconstructed_content.splitlines()
        if len(original_lines_for_debug) != len(reconstructed_lines_for_debug):
            logging.error(f"Line count mismatch: Original {len(original_lines_for_debug)}, Reconstructed {len(reconstructed_lines_for_debug)}")
        else:
            for i, (orig_line, recon_line) in enumerate(zip(original_lines_for_debug, reconstructed_lines_for_debug)):
                if orig_line != recon_line:
                    logging.error(f"Mismatch at line {i}:")
                    logging.error(f"  ORIG: '{orig_line}'")
                    logging.error(f"  RECON:'{recon_line}'")
                    break # Stop at first differing line
        return False, normalized_original_content, reconstructed_content

def get_crammed_article_ids_from_csv(csv_path, max_ids):
    """Reads article_db_id from the specified CSV file."""
    ids = set()
    if not os.path.exists(csv_path):
        logging.warning(f"Crammed articles CSV not found at {csv_path}. Skipping.")
        return list(ids)
    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            if 'article_db_id' not in reader.fieldnames:
                logging.error(f"'article_db_id' column not found in {csv_path}. Cannot select crammed articles.")
                return list(ids)
            for row in reader:
                try:
                    ids.add(int(row['article_db_id']))
                except ValueError:
                    logging.warning(f"Skipping invalid article_db_id in CSV: {row['article_db_id']}")
                if len(ids) >= max_ids:
                    break
        logging.info(f"Loaded {len(ids)} unique article IDs from {csv_path} (max requested: {max_ids}).")
    except Exception as e:
        logging.error(f"Error reading CSV {csv_path}: {e}")
    return list(ids)

def get_all_article_ids_from_db(conn):
    """Fetches all article IDs from the database."""
    cursor = conn.cursor()
    try:
        cursor.execute(f"SELECT {ID_COLUMN_NAME} FROM {TABLE_NAME}")
        return [row[0] for row in cursor.fetchall()]
    except sqlite3.Error as e:
        logging.error(f"Database error fetching all article IDs: {e}")
        return []

def main():
    logging.info("Starting article reconstruction test.")
    conn = None
    
    # Example content for direct testing (without DB) - keeping these for basic sanity checks
    example_content_preamble_articles_trailing = """This is a preamble.
It has multiple lines.

Άρθρο 1
This is article 1 content.
More content for article 1.

 Άρθρο 2 
This is article 2 content.

This is some trailing text after the last article.
It should be part of article 2."""

    example_content_only_preamble = """This is just a regular text document.
No article headers here.
"""

    example_content_only_one_article = """Άρθρο 10
Content for article 10.
"""
    example_content_empty = """"""
    example_content_newlines = """\n\n"""


    test_cases_direct = {
        "preamble_articles_trailing": example_content_preamble_articles_trailing,
        "only_preamble": example_content_only_preamble,
        "only_one_article": example_content_only_one_article,
        "empty_string": example_content_empty,
        "newlines_only": example_content_newlines
    }

    logging.info("--- Running direct string tests ---")
    direct_tests_failed = 0
    for name, content_str in test_cases_direct.items():
        logging.info(f"Direct test case: {name}")
        success, _, _ = test_reconstruction(f"direct_test_{name}", content_str)
        if not success:
            logging.error(f"Direct test case '{name}' FAILED.")
            direct_tests_failed +=1
    if direct_tests_failed > 0:
        logging.warning(f"{direct_tests_failed} direct string tests FAILED.")
    else:
        logging.info("All direct string tests PASSED.")
    logging.info("--- Finished direct string tests ---")


    logging.info(f"--- Running DB tests (DB: {DB_PATH}) ---")
    passed_count = 0
    failed_count = 0
    
    test_article_ids_final = []

    try:
        if not os.path.exists(DB_PATH):
            logging.error(f"Database file not found at {DB_PATH}. Skipping DB tests.")
            return

        conn = sqlite3.connect(DB_PATH)
        
        # 1. Get IDs from CSV
        crammed_ids = get_crammed_article_ids_from_csv(CRAMMED_ARTICLES_CSV_PATH, MAX_CRAMMED_TO_INCLUDE)
        test_article_ids_final.extend(crammed_ids)
        
        # 2. Get all IDs from DB
        all_db_ids = get_all_article_ids_from_db(conn)
        if not all_db_ids:
            logging.error("No article IDs found in the database. Cannot select random articles.")
            # Proceed with crammed_ids if any, otherwise exit
            if not test_article_ids_final:
                return
        
        # 3. Get remaining random IDs from DB
        remaining_needed = TOTAL_ARTICLES_TO_TEST - len(test_article_ids_final)
        if remaining_needed > 0 and all_db_ids:
            potential_random_ids = [id_val for id_val in all_db_ids if id_val not in test_article_ids_final]
            if len(potential_random_ids) < remaining_needed:
                logging.warning(f"Not enough unique random articles available. Will test {len(potential_random_ids)} random articles instead of {remaining_needed}.")
                test_article_ids_final.extend(potential_random_ids)
            else:
                test_article_ids_final.extend(random.sample(potential_random_ids, remaining_needed))
        
        if not test_article_ids_final:
            logging.warning("No articles selected for testing (neither from CSV nor DB). Exiting DB tests.")
            return

        logging.info(f"Will test a total of {len(test_article_ids_final)} unique article IDs: {test_article_ids_final}")

        for article_id in test_article_ids_final:
            original_content = fetch_article_content_by_id(conn, article_id)
            if original_content is not None:
                success, _, _ = test_reconstruction(article_id, original_content)
                if success:
                    passed_count += 1
                else:
                    failed_count += 1
            else:
                failed_count += 1 
                logging.warning(f"Could not fetch content for DB ID {article_id} to test.")

    except sqlite3.Error as e:
        logging.error(f"Database error during main execution: {e}")
    except Exception as e:
        logging.error(f"An unexpected error occurred: {e}", exc_info=True)
    finally:
        if conn:
            conn.close()
            logging.info("Database connection closed.")
            
    logging.info(f"--- DB Test Summary ---")
    logging.info(f"Total DB articles tested: {passed_count + failed_count} (out of {len(test_article_ids_final)} selected)")
    logging.info(f"Passed: {passed_count}")
    logging.info(f"Failed: {failed_count}")
    logging.info("Finished article reconstruction test.")

if __name__ == "__main__":
    main() 