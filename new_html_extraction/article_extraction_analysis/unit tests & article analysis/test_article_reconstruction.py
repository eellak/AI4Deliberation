#!/usr/bin/env python3
"""
Test script to verify that article content can be perfectly reconstructed
from the chunks generated by article_parser_utils.extract_all_main_articles_with_content.
"""

import sqlite3
import logging
import os
import csv # Added for CSV reading
import random # Added for random selection
import unittest
import sys
import json
import pandas as pd # Added for CSV reading

# Correctly add the path to article_parser_utils.py
# Assuming the script is run from the workspace root /mnt/data/AI4Deliberation
# or that os.getcwd() during test execution correctly reflects a known base
WORKSPACE_ROOT = "/mnt/data/AI4Deliberation" # Hardcode for clarity if needed, or derive carefully
# For robustness if __file__ is available and reliable:
# current_script_dir = os.path.dirname(os.path.abspath(__file__))
# utils_dir_path = os.path.abspath(os.path.join(current_script_dir, "..")) # This goes up to article_extraction_analysis

# Let's try a path relative to a known structure, assuming the test is in 
# .../unit tests & article analysis/
# and utils are in .../ (one level up from tests folder)
utils_dir_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

if utils_dir_path not in sys.path:
    sys.path.insert(0, utils_dir_path)

# Check if the path leads to the expected file as a debug step
# expected_util_file = os.path.join(utils_dir_path, "article_parser_utils.py")
# if not os.path.exists(expected_util_file):
#     print(f"DEBUG: article_parser_utils.py not found at {expected_util_file}", file=sys.stderr)
#     print(f"DEBUG: Current sys.path: {sys.path}", file=sys.stderr)
#     # Attempt an alternative path based on a fixed workspace root if the above fails contextually
#     # This is a fallback if __file__ context is tricky
#     fixed_utils_path = "/mnt/data/AI4Deliberation/new_html_extraction/article_extraction_analysis"
#     if fixed_utils_path not in sys.path:
#         sys.path.insert(0, fixed_utils_path)
#         print(f"DEBUG: Added fixed path {fixed_utils_path} to sys.path", file=sys.stderr)

# Assuming article_parser_utils is in the same directory or accessible in PYTHONPATH
from article_parser_utils import extract_all_main_articles_with_content, extract_article_sequences

# --- Configuration ---
# Use the database that we confirmed has data
DB_PATH = os.path.abspath(os.path.join(utils_dir_path, "..", "deliberation_data_gr_markdownify.db"))
TABLE_NAME = "articles"
CONTENT_COLUMN_NAME = "content"
ID_COLUMN_NAME = "id"
CRAMMED_ARTICLES_CSV_PATH = "/mnt/data/AI4Deliberation/new_html_extraction/article_extraction_analysis/crammed_articles_integrity_report_enhanced.csv" # Added CSV path
TOTAL_ARTICLES_TO_TEST = 20 # Target number of articles to test
MAX_CRAMMED_TO_INCLUDE = 10 # Max articles to pick from the CSV

# Configure logging
LOG_FORMAT = '%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'
logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)

def fetch_article_content_by_id(conn, article_db_id):
    """Fetches the content of a specific article by its DB ID."""
    cursor = conn.cursor()
    try:
        query = f"SELECT {CONTENT_COLUMN_NAME} FROM {TABLE_NAME} WHERE {ID_COLUMN_NAME} = ?"
        cursor.execute(query, (article_db_id,))
        row = cursor.fetchone()
        if row:
            return row[0]
        else:
            logging.warning(f"Article content not found for DB ID: {article_db_id}")
            return None
    except sqlite3.Error as e:
        logging.error(f"Database error fetching content for DB ID {article_db_id}: {e}")
        return None

def test_reconstruction(article_db_id, original_content):
    """
    Tests if the original_content can be reconstructed from chunks.
    """
    if original_content is None:
        logging.warning(f"Skipping reconstruction test for DB ID {article_db_id} due to None content.")
        return False, None, None # Indicate failure but also that original was None

    logging.info(f"--- Testing Reconstruction for Article DB ID: {article_db_id} ---")
    
    chunks = extract_all_main_articles_with_content(original_content)
    
    if not chunks:
        # If original_content was not empty but extract_all_main_articles_with_content returned no chunks,
        # this could be a problem unless original_content was truly empty (e.g., "" or just newlines that result in empty lines list).
        # The modified extract_all_main_articles_with_content should return a preamble chunk for non-empty content without article headers.
        # If original_content was "" -> lines=[] -> chunks=[] -> reconstructed="". This is correct.
        # If original_content was "\n" -> lines=[''] -> chunks=[{'type':'preamble', 'content_text':'', ...}] -> reconstructed="". This is also correct.
        logging.info(f"No chunks extracted for DB ID {article_db_id}. Original content might be effectively empty.")
        reconstructed_content = ""
    else:
        reconstructed_content = "\n".join(chunk['content_text'] for chunk in chunks)

    # Normalize original content by splitting and rejoining lines, similar to how chunks are processed and joined.
    # This handles cases where original content might have different newline styles (e.g. \r\n) or trailing newlines
    # not perfectly matched by the reconstruction of lines.splitlines() and then \n.join().
    normalized_original_content = "\n".join(original_content.splitlines())

    if normalized_original_content == reconstructed_content:
        logging.info(f"SUCCESS: Article DB ID {article_db_id} reconstructed perfectly.")
        return True, normalized_original_content, reconstructed_content
    else:
        logging.error(f"FAILURE: Article DB ID {article_db_id} reconstruction mismatch.")
        # For debugging, print original and reconstructed if they differ significantly
        # To avoid overwhelming logs, maybe just print lengths or a snippet.
        logging.debug(f"Original (normalized, len {len(normalized_original_content)}):\n'''{normalized_original_content[:500]}...'''")
        logging.debug(f"Reconstructed (len {len(reconstructed_content)}):\n'''{reconstructed_content[:500]}...'''")
        
        # Further debugging: Check line by line if lengths are similar
        original_lines_for_debug = normalized_original_content.splitlines()
        reconstructed_lines_for_debug = reconstructed_content.splitlines()
        if len(original_lines_for_debug) != len(reconstructed_lines_for_debug):
            logging.error(f"Line count mismatch: Original {len(original_lines_for_debug)}, Reconstructed {len(reconstructed_lines_for_debug)}")
        else:
            for i, (orig_line, recon_line) in enumerate(zip(original_lines_for_debug, reconstructed_lines_for_debug)):
                if orig_line != recon_line:
                    logging.error(f"Mismatch at line {i}:")
                    logging.error(f"  ORIG: '{orig_line}'")
                    logging.error(f"  RECON:'{recon_line}'")
                    break # Stop at first differing line
        return False, normalized_original_content, reconstructed_content

def get_crammed_article_ids_from_csv(csv_path, max_ids):
    """Reads article_db_id from the specified CSV file."""
    ids = set()
    if not os.path.exists(csv_path):
        logging.warning(f"Crammed articles CSV not found at {csv_path}. Skipping.")
        return list(ids)
    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            if 'article_db_id' not in reader.fieldnames:
                logging.error(f"'article_db_id' column not found in {csv_path}. Cannot select crammed articles.")
                return list(ids)
            for row in reader:
                try:
                    ids.add(int(row['article_db_id']))
                except ValueError:
                    logging.warning(f"Skipping invalid article_db_id in CSV: {row['article_db_id']}")
                if len(ids) >= max_ids:
                    break
        logging.info(f"Loaded {len(ids)} unique article IDs from {csv_path} (max requested: {max_ids}).")
    except Exception as e:
        logging.error(f"Error reading CSV {csv_path}: {e}")
    return list(ids)

def get_all_article_ids_from_db(conn):
    """Fetches all article IDs from the database."""
    cursor = conn.cursor()
    try:
        cursor.execute(f"SELECT {ID_COLUMN_NAME} FROM {TABLE_NAME}")
        return [row[0] for row in cursor.fetchall()]
    except sqlite3.Error as e:
        logging.error(f"Database error fetching all article IDs: {e}")
        return []

def main():
    logging.info("Starting article reconstruction test.")
    conn = None
    
    # Example content for direct testing (without DB) - keeping these for basic sanity checks
    example_content_preamble_articles_trailing = """This is a preamble.
It has multiple lines.

Άρθρο 1
This is article 1 content.
More content for article 1.

 Άρθρο 2 
This is article 2 content.

This is some trailing text after the last article.
It should be part of article 2."""

    example_content_only_preamble = """This is just a regular text document.
No article headers here.
"""

    example_content_only_one_article = """Άρθρο 10
Content for article 10.
"""
    example_content_empty = """"""
    example_content_newlines = """\n\n"""


    test_cases_direct = {
        "preamble_articles_trailing": example_content_preamble_articles_trailing,
        "only_preamble": example_content_only_preamble,
        "only_one_article": example_content_only_one_article,
        "empty_string": example_content_empty,
        "newlines_only": example_content_newlines
    }

    logging.info("--- Running direct string tests ---")
    direct_tests_failed = 0
    for name, content_str in test_cases_direct.items():
        logging.info(f"Direct test case: {name}")
        success, _, _ = test_reconstruction(f"direct_test_{name}", content_str)
        if not success:
            logging.error(f"Direct test case '{name}' FAILED.")
            direct_tests_failed +=1
    if direct_tests_failed > 0:
        logging.warning(f"{direct_tests_failed} direct string tests FAILED.")
    else:
        logging.info("All direct string tests PASSED.")
    logging.info("--- Finished direct string tests ---")


    logging.info(f"--- Running DB tests (DB: {DB_PATH}) ---")
    passed_count = 0
    failed_count = 0
    
    test_article_ids_final = []

    try:
        if not os.path.exists(DB_PATH):
            logging.error(f"Database file not found at {DB_PATH}. Skipping DB tests.")
            return

        conn = sqlite3.connect(DB_PATH)
        
        # 1. Get IDs from CSV
        crammed_ids = get_crammed_article_ids_from_csv(CRAMMED_ARTICLES_CSV_PATH, MAX_CRAMMED_TO_INCLUDE)
        test_article_ids_final.extend(crammed_ids)
        
        # 2. Get all IDs from DB
        all_db_ids = get_all_article_ids_from_db(conn)
        if not all_db_ids:
            logging.error("No article IDs found in the database. Cannot select random articles.")
            # Proceed with crammed_ids if any, otherwise exit
            if not test_article_ids_final:
                return
        
        # 3. Get remaining random IDs from DB
        remaining_needed = TOTAL_ARTICLES_TO_TEST - len(test_article_ids_final)
        if remaining_needed > 0 and all_db_ids:
            potential_random_ids = [id_val for id_val in all_db_ids if id_val not in test_article_ids_final]
            if len(potential_random_ids) < remaining_needed:
                logging.warning(f"Not enough unique random articles available. Will test {len(potential_random_ids)} random articles instead of {remaining_needed}.")
                test_article_ids_final.extend(potential_random_ids)
            else:
                test_article_ids_final.extend(random.sample(potential_random_ids, remaining_needed))
        
        if not test_article_ids_final:
            logging.warning("No articles selected for testing (neither from CSV nor DB). Exiting DB tests.")
            return

        logging.info(f"Will test a total of {len(test_article_ids_final)} unique article IDs: {test_article_ids_final}")

        for article_id in test_article_ids_final:
            original_content = fetch_article_content_by_id(conn, article_id)
            if original_content is not None:
                success, _, _ = test_reconstruction(article_id, original_content)
                if success:
                    passed_count += 1
                else:
                    failed_count += 1
            else:
                failed_count += 1 
                logging.warning(f"Could not fetch content for DB ID {article_id} to test.")

    except sqlite3.Error as e:
        logging.error(f"Database error during main execution: {e}")
    except Exception as e:
        logging.error(f"An unexpected error occurred: {e}", exc_info=True)
    finally:
        if conn:
            conn.close()
            logging.info("Database connection closed.")
            
    logging.info(f"--- DB Test Summary ---")
    logging.info(f"Total DB articles tested: {passed_count + failed_count} (out of {len(test_article_ids_final)} selected)")
    logging.info(f"Passed: {passed_count}")
    logging.info(f"Failed: {failed_count}")
    logging.info("Finished article reconstruction test.")

class TestArticleReconstruction(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        """Set up resources for all tests in this class."""
        cls.conn = None
        if not os.path.exists(DB_PATH):
            logging.warning(f"DB not found at {DB_PATH}. DB-dependent tests will be skipped or will fail if not handled.")
            return
        try:
            cls.conn = sqlite3.connect(DB_PATH)
        except sqlite3.Error as e:
            logging.error(f"Failed to connect to DB at {DB_PATH}: {e}")

    @classmethod
    def tearDownClass(cls):
        """Clean up resources after all tests in this class."""
        if cls.conn:
            cls.conn.close()
            logging.info("DB connection closed by tearDownClass.")

    def _fetch_article_content_by_id(self, article_db_id):
        """Fetches the content of a specific article by its DB ID."""
        if not self.conn:
            self.skipTest("DB connection not available, skipping DB-dependent part of test.")
            return None
            
        cursor = self.conn.cursor()
        try:
            query = f"SELECT {CONTENT_COLUMN_NAME} FROM {TABLE_NAME} WHERE {ID_COLUMN_NAME} = ?"
            cursor.execute(query, (article_db_id,))
            row = cursor.fetchone()
            return row[0] if row else None
        except sqlite3.Error as e:
            logging.error(f"DB error fetching content for ID {article_db_id}: {e}")
            return None

    def _reconstruct_content_from_chunks(self, chunks):
        """Helper to reconstruct content from a list of parsed chunks."""
        reconstructed_lines = []
        for chunk in chunks:
            reconstructed_lines.append(chunk['content_text'])
        return "\n".join(reconstructed_lines)

    def test_reconstruction_from_extract_all_main_articles_static(self):
        """Tests reconstruction with static string examples."""
        test_cases = [
            {"name": "No articles", "content": "Just some random text.\nAnother line."},
            {"name": "Single main article", "content": "Άρθρο 1\nThis is content for article 1.\nMore content."},
            {"name": "Two main articles", "content": "Άρθρο 1\nContent 1\nΆρθρο 2\nContent 2"},
            {"name": "Preamble and one article", "content": "This is a preamble.\nIt has multiple lines.\nΆρθρο 1\nContent for article 1."},
            {"name": "Empty content", "content": ""},
            {"name": "Content with only newlines", "content": "\n\n\n"},
        ]
        for case in test_cases:
            with self.subTest(name=case["name"]):
                original_content = case["content"]
                expected_reconstructed_content = "\n".join(original_content.splitlines())
                chunks = extract_all_main_articles_with_content(original_content)
                reconstructed_content = self._reconstruct_content_from_chunks(chunks)
                self.assertEqual(reconstructed_content, expected_reconstructed_content)

    def test_reconstruction_from_db_articles_range_1_to_200(self):
        """Tests reconstruction for article DB IDs 1 to 200."""
        if not self.conn:
            self.skipTest(f"Database not available or connection failed (path: {DB_PATH}). Skipping DB reconstruction test.")

        test_article_ids = range(1, 201) # IDs 1 to 200 inclusive
        logging.info(f"Testing DB article reconstruction for IDs 1 to 200.")

        found_count = 0
        for article_id in test_article_ids:
            with self.subTest(article_db_id=article_id):
                original_content = self._fetch_article_content_by_id(article_id)
                if original_content is None:
                    logging.info(f"Article DB ID {article_id} not found in DB. Skipping.")
                    continue # Skip if article doesn't exist
                
                found_count += 1
                expected_reconstructed_content = "\n".join(original_content.splitlines())
                chunks = extract_all_main_articles_with_content(original_content)
                reconstructed_content = self._reconstruct_content_from_chunks(chunks)
                self.assertEqual(reconstructed_content, expected_reconstructed_content,
                                 msg=f"Reconstruction failed for DB ID {article_id}")
        logging.info(f"Tested {found_count} articles found within DB ID range 1-200.")
        self.assertTrue(found_count > 0, "No articles in range 1-200 were found in the database to test.")

    def test_reconstruction_from_v4_report_json(self):
        """Tests reconstruction from the article_structure_json in the v4 report."""
        v4_report_path = os.path.join(utils_dir_path, "crammed_articles_integrity_report_enhanced_v4.csv")
        self.assertTrue(os.path.exists(v4_report_path), f"v4 report not found at {v4_report_path}")

        try:
            df = pd.read_csv(v4_report_path)
        except Exception as e:
            self.fail(f"Failed to read CSV {v4_report_path}: {e}")

        test_row_df = df[(df['consultation_id'] == 1) & (df['article_db_id'] == 2)]
        if test_row_df.empty:
            test_row_df = df[df['article_structure_json'].notna() & (df['article_structure_json'] != '')].head(1)
        
        self.assertFalse(test_row_df.empty, "No suitable test row with article_structure_json found in v4 report.")
        test_row = test_row_df.iloc[0]
        article_db_id_for_test = test_row['article_db_id']
        original_content_from_db_col = test_row['db_entry_content']
        json_string = test_row['article_structure_json']

        self.assertIsInstance(json_string, str, "article_structure_json is not a string.")
        self.assertTrue(len(json_string) > 0, "article_structure_json is empty.")

        try:
            parsed_json_chunks = json.loads(json_string)
        except json.JSONDecodeError as e:
            self.fail(f"Failed to parse JSON for article_db_id {article_db_id_for_test}: {e}")

        reconstructed_from_json_lines = [chunk['content_text'] for chunk in parsed_json_chunks]
        reconstructed_content_from_json = "\n".join(reconstructed_from_json_lines)
        expected_original_content = "\n".join(original_content_from_db_col.splitlines())
        self.assertEqual(reconstructed_content_from_json, expected_original_content)

if __name__ == '__main__':
    unittest.main() 